{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ff66731e61db933"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Accuracy and Dummy Model\n",
    "\n",
    "1. number of correct predictions / total predictions\n",
    "2. A specific probability where the prediction changes from 0 to 1 class\n",
    "3. .5\n",
    "4. from sklearn.metrics import accuracy_score\n",
    "5. A dummy model is a model where the positive class is always predicted and serves as a baseline for how other models perform against this model, which is considered to be guessing.\n",
    "6. Class imbalance is when the positive and negative classes aren't around the same amount in the dataset. In this case it's something like a 73% imbalance.\n",
    "7. Accuracy is misleading when there is a class imbalance because we learned from the dummy model that you can be 73% accurate just by guessing due to the class imbalance. The class imbalance gives a false impression that the accuracy is properly earned.\n",
    "\n",
    "\n",
    "## Confusion Table, Precision and Recall\n",
    "\n",
    "1. FP, TP, FN, TN\n",
    "2. Precision is the proportion of correct positives out of all positives. TP/(TP + FP)\n",
    "3. Recall (sensitivity) is the proportion of correct positives out of actual positives. TP/(TP + FN)\n",
    "4. True positive rate\n",
    "5. False positive rate\n",
    "\n",
    "\n",
    "## ROC Curve\n",
    "\n",
    "1. binary\n",
    "2. TP and FP\n",
    "3. recall\n",
    "4. thresholds\n",
    "5. FPR, TPR\n",
    "6. straight diagonal\n",
    "7. the black(white) lines are the tpr and fpr for the ideal model. The ideal model represents negative and then positive observations in the proportion of the original dataset. It creates a dataset that has 100% accuracy. The blue and orange lines are the output of the real model.\n",
    "8. Receiver operator characteristic\n",
    "9. evaluating the diagnostic performance of a test or classification model by illustrating the trade-off between its true positive rate and false positive rate across all possible thresholds\n",
    "10. Above the 50% diagonal, as close to 1 as possible.\n",
    "11. plt\n",
    "\n",
    "\n",
    "## ROC AUC\n",
    "\n",
    "1. a classifier's ability to distinguish between classes by plotting the true positive rate against the false positive rate at various thresholds\n",
    "2. FPR, TPR\n",
    "3. 1 and .5\n",
    "4. from sklearn.metrics import auc\n",
    "5. the probability that a randomly selected positive sample will be ranked higher than a randomly selected negative sample by the model\n",
    "\n",
    "\n",
    "## Cross-Validation\n",
    "\n",
    "1. train and validate\n",
    "2. Cross-validation splits the dataset up into 3 parts and all parts are used in turn as the validation dataset. For example, when 1 and 2 are used to train, 3 is used to validate. Same with 2 and 3 being used to train and 1 being used to validate.\n",
    "3. folds\n",
    "4. mean, standard deviation of AUC scores\n",
    "5. stable\n",
    "6. from sklearn.model_selection import KFold\n",
    "7. cross-validation can be thought of as a loop that encompasses the standard binary classification procedure. The process of binary classification is repeated using various folds, and the resulting AUC scores are used to show the stability of the model"
   ],
   "id": "6b5632de83bc54c6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
